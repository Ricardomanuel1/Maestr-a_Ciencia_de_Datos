{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ricardomanuel1/Maestria_Ciencia_de_Datos/blob/main/MACHINE%20LEARNING%20Y%20DEEP%20LEARNING/cnns_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImageNet dataset\n",
        "\n",
        "ImageNet es una de las bases de datos más grandes y ampliamente utilizadas para\n",
        "el entrenamiento y evaluación de algoritmos de visión por computadora, particularmente\n",
        "en tareas de clasificación de imágenes y reconocimiento de objetos. Fue creada por un\n",
        "equipo liderado por Fei-Fei Li y lanzada en 2009.\n",
        "\n",
        "Características de la Base de Datos ImageNet:\n",
        "\n",
        "* **Número de Imágenes**: ImageNet contiene más de 14 millones de imágenes, con anotaciones para más de 20,000 categorías de objetos.\n",
        "* **Conjunto de Entrenamiento**: 1,281,167 imágenes images.\n",
        "* **Conjunto de Valadación**: 50,000 imágenes.\n",
        "* **Conjunto de Test**: 100,000 imágenes.\n",
        "* **Dimensiones de las Imágenes**: Cada imagen es de 28x28 píxeles.\n",
        "* **Formato de las Imágenes**: A color (tres canales).\n",
        "* **Etiquetas**: Uno de los aspectos más destacados de ImageNet es el ILSVRC, una\n",
        "competencia anual que desafía a los investigadores a desarrollar los mejores algoritmos\n",
        "para la clasificación y detección de objetos en un subconjunto de 1,000 categorías de ImageNet."
      ],
      "metadata": {
        "id": "cMSbEWPPUA5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo VGG16\n",
        "\n",
        "VGG16 es una de las arquitecturas de redes neuronales convolucionales (CNN) más influyentes y ampliamente utilizadas en la visión por computadora. Fue desarrollada por Simonyan y Zisserman del Visual Geometry Group (VGG) de la Universidad de Oxford y presentada en el artículo \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" en 2014.\n",
        "Todos los kernel son de 3x3\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oYJMdepcSIkkUjsSp8Y3XtnGVVpSiuQG)"
      ],
      "metadata": {
        "id": "hF9VcZYsURRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Arquitectura del Modelo VGG16\n",
        "\n",
        "### 1.1. Capa de Entrada:\n",
        "\n",
        "* Dimensión: 224x224X3 píxeles (imagen en escala de grises).\n",
        "* Nota: Las imágenes de IMAGENET originalmente son de 28x28X3 píxeles, por lo que se * agregan bordes para aumentar a 224x224 píxeles.\n",
        "\n",
        "### 1.2. Capa Convolucional **conv1**:\n",
        "\n",
        "* Filtros: 64 filtros de 3x3.\n",
        "* Salida: 64 mapas de características de 224x224 (224-3+2 + 1 = 224).\n",
        "          224x224x64\n",
        "          tiene padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 1.3. Capa Convolucional **conv2**:\n",
        "\n",
        "* Filtros: 64 filtros de 3x3.\n",
        "* Salida: 64 mapas de características de 224x224 (224-3+2+1 =224).\n",
        "          224x224x64\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 1.4. Capa de Subsampling **s1**:\n",
        "\n",
        "* Operación: Subsampling o pooling (promedio) con una ventana de 2x2.\n",
        "* Salida: 128 mapas de características de 112x112.\n",
        "          112x112x128\n",
        "          Entonces se reduce de 224x224 a 112x112, pero con nueva\n",
        "          profundidad 128\n",
        "      \n",
        "\n",
        "### 1.5. Capa Convolucional **conv3**:\n",
        "\n",
        "* Filtros: 128 filtros de 3x3\n",
        "* Salida: 128 mapas de características de 112x112 (112-3+2+1=112).\n",
        "          112x112x128\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 1.6. Capa Convolucional **conv4**:\n",
        "\n",
        "* Filtros: 128 filtros de 3x3\n",
        "* Salida: 128 mapas de características de 112x112 (112-3+2+1=112).\n",
        "          112x112x128\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 1.7. Capa de Subsampling **s2**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 256 mapas de características de 56x56.\n",
        "          56x56x256\n",
        "\n",
        "### 1.8. Capa Convolucional **conv5**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3  \n",
        "\n",
        "### 1.9. Capa Convolucional **conv6**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3\n",
        "\n",
        "### 2.0. Capa Convolucional **conv7**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3\n",
        "\n",
        "### 2.1. Capa de Subsampling **s3**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 512 mapas de características de 28x28.\n",
        "          28x28x512\n",
        "\n",
        "### 2.2. Capa Convolucional **conv8**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.3. Capa Convolucional **conv9**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.4. Capa Convolucional **conv10**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.5. Capa Convolucional **conv11**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 ((28-3+2)/2+1=14).\n",
        "          14x14x512\n",
        "          padding=1  strike=2 kernel=3\n",
        "\n",
        "\n",
        "### 2.6. Capa Convolucional **conv12**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "\n",
        "### 2.7. Capa Convolucional **conv13**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 2.8. Capa Convolucional **conv14**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 2.9. Capa de Subsampling **s4**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 512 mapas de características de 7x7.\n",
        "          7x7x512\n",
        "\n",
        "### 2.10. Capa Completamente Conectada **f1**:\n",
        "\n",
        "* Neuronas: 4096.\n",
        "* Operación: Función de activación relu.\n",
        "\n",
        "### 2.11. Capa Completamente Conectada **f2**:\n",
        "\n",
        "* Neuronas: 4096.\n",
        "* Operación: Función de activación relu.\n",
        "\n",
        "\n",
        "### 2.11 Capa Completamente Conectada **f2** (salida):\n",
        "\n",
        "* Neuronal: 1000 (una por cada dígito del 0 al 999).\n",
        "* Operación: Función de activación **softmax** para clasificación.\n",
        "\n",
        "\n",
        "\n",
        "### 1.2. Flujo de Datos en la Red\n",
        "* Entrada: Imagen de 224x224x3 píxeles.\n",
        "* **conv1**: Convolución → 224x224x64.  64 filtros\n",
        "* **conv2**: Convolución → 224x224x64.\n",
        "* **s1**: Subsampling → 112x112x128.    128 filtros\n",
        "* **conv3**: Convolución → 112x112x128.  128 filtros\n",
        "* **conv4**: Convolución → 112x112x128.  128 filtros\n",
        "* **s2**: Subsampling → 56x56x256.    128 filtros\n",
        "* **conv5**: Convolución → 56x56x256.  256 filtros\n",
        "* **conv6**: Convolución → 56x56x256.  256 filtros\n",
        "* **conv7**: Convolución → 56x56x256.  256 filtros\n",
        "* **s3**: Subsampling → 28x28x512.   512 filtros\n",
        "* **conv8**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv9**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv10**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv11**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv12**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv13**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv14**: Convolución → 14x14x512.  512 filtros\n",
        "* **s4**: Subsampling → 7x7x512.   512 filtros\n",
        "\n",
        "* **f1**: Conexión completa → 4096.\n",
        "* **f2**: Conexión completa → 4096.\n",
        "* **f3**: Conexión completa → 1000 (clases)."
      ],
      "metadata": {
        "id": "e_M-8huuJWHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "f4d4HeYzTcCv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#por defecto padding es 0 y stride 1  (3,64,3,0,1) abreviado (3,64,3)\n",
        "#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                #dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        # conv1: entrada 3 canal, salida 64 canales, tamaño de kernel o filtro 3x3,padding=1, strike=1\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1)\n",
        "\n",
        "        # conv2: entrada 64 canales, salida 64 canales, tamaño de kernel 3x3,padding=1\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        #return x\n",
        "\n",
        "        # conv3: entrada 64 canales, salida 128 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv3 = nn.Conv2d(64, 128,3, padding=1)\n",
        "\n",
        "        # conv4: entrada 128 canales, salida 128 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv4 = nn.Conv2d(128, 128,3, padding=1)\n",
        "\n",
        "        # conv5: entrada 128 canales, salida 256 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv5 = nn.Conv2d(128, 256,3, padding=1)\n",
        "\n",
        "        # conv6: entrada 256 canales, salida 256 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv6 = nn.Conv2d(256, 256,3, padding=1)\n",
        "\n",
        "        # conv7: entrada 256 canales, salida 256 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv7 = nn.Conv2d(256, 256,3, padding=1)\n",
        "\n",
        "        # conv8: entrada 256 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv8 = nn.Conv2d(256, 512,3, padding=1)\n",
        "\n",
        "        # conv9: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv9 = nn.Conv2d(512, 512,3, padding=1)\n",
        "\n",
        "        # conv10: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv10 = nn.Conv2d(512, 512,3, padding=1)\n",
        "\n",
        "        # conv11: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1,stride=2\n",
        "        self.conv11 = nn.Conv2d(512, 512,3, padding=1, stride=2)\n",
        "\n",
        "        # conv12: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv12 = nn.Conv2d(512, 512,3, padding=1)\n",
        "\n",
        "        # conv13: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv13 = nn.Conv2d(512, 512,3, padding=1)\n",
        "\n",
        "        # conv14: entrada 512 canales, salida 512 canales, tamaño de kernel 3x3, padding 1\n",
        "        self.conv14 = nn.Conv2d(512, 512,3, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "        # f1: entrada 7x7x512=25088, salida 4096\n",
        "        self.fc1 = nn.Linear(25088,4096)\n",
        "\n",
        "        # f2: entrada 4096, salida 4096\n",
        "        self.fc2 = nn.Linear(4096,4096)\n",
        "\n",
        "        # f3: entrada 4096, salida 1000 clases\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicar la primera capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # Aplicar la segunda capa convolucional seguida de ReLU y max pooling\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Aplicar la tercera capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Aplicar la cuarta capa convolucional seguida de ReLU y max pooling\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Aplicar la quinta capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv5(x))\n",
        "\n",
        "        # Aplicar la sexta capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv6(x))\n",
        "\n",
        "        # Aplicar la septima capa convolucional seguida de ReLU y max pooling\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Aplicar la octava capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv8(x))\n",
        "\n",
        "        # Aplicar la novena capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv9(x))\n",
        "\n",
        "        # Aplicar la decima capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv10(x))\n",
        "\n",
        "        # Aplicar la onceava capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv11(x))\n",
        "\n",
        "        # Aplicar la doceava capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv12(x))\n",
        "\n",
        "        # Aplicar la treceava capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv13(x))\n",
        "\n",
        "        # Aplicar la catorceava capa convolucional seguida de ReLU y  max pooling\n",
        "        x = F.relu(self.conv14(x))\n",
        "        x = F.max_pool2d(x, 2)       #7x7x512\n",
        "\n",
        "        # Aplanar los datos para la capa completamente conectada\n",
        "        x = x.view(-1, 25088)\n",
        "\n",
        "        # Aplicar la primera capa completamente conectada seguida de ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "\n",
        "        # Aplicar la segunda capa completamente conectada seguida de RELU\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "\n",
        "        # Aplicar la tercera capa completamente conectada\n",
        "        x = F.relu(self.fc3(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kgTe12RG0Y92"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CocN85NFOTHG",
        "outputId": "8bdf2774-292f-47cc-fc7e-7f6a755f1ef0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un batch de 100 imágenes de 224x224x3\n",
        "\n",
        "x = torch.randn(100,3,224,224)"
      ],
      "metadata": {
        "id": "wWU6VCsLy_0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alimentar el modelo VGG16 con el batch de imágenes\n",
        "\n",
        "y = model(x)"
      ],
      "metadata": {
        "id": "VBXqZ70EzDrd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "metadata": {
        "id": "KdsSohIXZ8UZ",
        "outputId": "25a1e60c-31cf-4a7e-db0c-b6e3619f32de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1000])\n"
          ]
        }
      ]
    }
  ]
}