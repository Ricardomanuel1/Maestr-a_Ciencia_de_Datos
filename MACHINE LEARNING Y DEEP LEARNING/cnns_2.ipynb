{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ricardomanuel1/Maestria_Ciencia_de_Datos/blob/main/MACHINE%20LEARNING%20Y%20DEEP%20LEARNING/cnns_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImageNet dataset\n",
        "\n",
        "ImageNet es una de las bases de datos más grandes y ampliamente utilizadas para\n",
        "el entrenamiento y evaluación de algoritmos de visión por computadora, particularmente\n",
        "en tareas de clasificación de imágenes y reconocimiento de objetos. Fue creada por un\n",
        "equipo liderado por Fei-Fei Li y lanzada en 2009.\n",
        "\n",
        "Características de la Base de Datos ImageNet:\n",
        "\n",
        "* **Número de Imágenes**: ImageNet contiene más de 14 millones de imágenes, con anotaciones para más de 20,000 categorías de objetos.\n",
        "* **Conjunto de Entrenamiento**: 1,281,167 imágenes images.\n",
        "* **Conjunto de Valadación**: 50,000 imágenes.\n",
        "* **Conjunto de Test**: 100,000 imágenes.\n",
        "* **Dimensiones de las Imágenes**: Cada imagen es de 28x28 píxeles.\n",
        "* **Formato de las Imágenes**: A color (tres canales).\n",
        "* **Etiquetas**: Uno de los aspectos más destacados de ImageNet es el ILSVRC, una\n",
        "competencia anual que desafía a los investigadores a desarrollar los mejores algoritmos\n",
        "para la clasificación y detección de objetos en un subconjunto de 1,000 categorías de ImageNet."
      ],
      "metadata": {
        "id": "cMSbEWPPUA5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo VGG16\n",
        "\n",
        "VGG16 es una de las arquitecturas de redes neuronales convolucionales (CNN) más influyentes y ampliamente utilizadas en la visión por computadora. Fue desarrollada por Simonyan y Zisserman del Visual Geometry Group (VGG) de la Universidad de Oxford y presentada en el artículo \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" en 2014.\n",
        "Todos los kernel son de 3x3\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oYJMdepcSIkkUjsSp8Y3XtnGVVpSiuQG)"
      ],
      "metadata": {
        "id": "hF9VcZYsURRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Arquitectura del Modelo VGG16\n",
        "\n",
        " VGG16\n",
        " **tiene una arquitectura sencilla y bien estructurada, que consta de 7 capas (excluyendo las capas de entrada y salida). Estas capas incluyen tres capas convolucionales, dos capas de subsampling (o pooling), y dos capas completamente conectadas.**\n",
        "\n",
        "### 1.1. Capa de Entrada:\n",
        "\n",
        "* Dimensión: 224x224X3 píxeles (imagen en escala de grises).\n",
        "* Nota: Las imágenes de IMAGENET originalmente son de 28x28X3 píxeles, por lo que se * agregan bordes para aumentar a 224x224 píxeles.\n",
        "\n",
        "### 1.2. Capa Convolucional **conv1**:\n",
        "\n",
        "* Filtros: 64 filtros de 3x3.\n",
        "* Salida: 64 mapas de características de 224x224 (224-3+2 + 1 = 224).\n",
        "          224x224x64\n",
        "          tiene padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 1.3. Capa Convolucional **conv2**:\n",
        "\n",
        "* Filtros: 64 filtros de 3x3.\n",
        "* Salida: 64 mapas de características de 224x224 (224-3+2+1 =224).\n",
        "          224x224x64\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 1.4. Capa de Subsampling **s1**:\n",
        "\n",
        "* Operación: Subsampling o pooling (promedio) con una ventana de 2x2.\n",
        "* Salida: 128 mapas de características de 112x112.\n",
        "          112x112x128\n",
        "          Entonces se reduce de 224x224 a 112x112, pero con nueva\n",
        "          profundidad 128\n",
        "      \n",
        "\n",
        "### 1.5. Capa Convolucional **conv3**:\n",
        "\n",
        "* Filtros: 128 filtros de 3x3\n",
        "* Salida: 128 mapas de características de 112x112 (112-3+2+1=112).\n",
        "          112x112x128\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 1.6. Capa Convolucional **conv4**:\n",
        "\n",
        "* Filtros: 128 filtros de 3x3\n",
        "* Salida: 128 mapas de características de 112x112 (112-3+2+1=112).\n",
        "          112x112x128\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 1.7. Capa de Subsampling **s2**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 256 mapas de características de 56x56.\n",
        "          56x56x256\n",
        "\n",
        "### 1.8. Capa Convolucional **conv5**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3  \n",
        "\n",
        "### 1.9. Capa Convolucional **conv6**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3\n",
        "\n",
        "### 2.0. Capa Convolucional **conv7**:\n",
        "\n",
        "* Filtros: 256 filtros de 3x3\n",
        "* Salida: 256 mapas de características de 56x56 (56-3+2+1=56).\n",
        "          56x56x256\n",
        "          padding=1  strike=1 kernel=3\n",
        "\n",
        "### 2.1. Capa de Subsampling **s3**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 512 mapas de características de 28x28.\n",
        "          28x28x512\n",
        "\n",
        "### 2.2. Capa Convolucional **conv8**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.3. Capa Convolucional **conv9**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.4. Capa Convolucional **conv10**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 28x28 (28-3+2+1=28).\n",
        "          28x28x512\n",
        "          padding=1  strike=1  kernel=3\n",
        "\n",
        "### 2.5. Capa Convolucional **conv11**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 ((28-3+2)/2+1=14).\n",
        "          14x14x512\n",
        "          padding=1  strike=2 kernel=3\n",
        "\n",
        "\n",
        "### 2.6. Capa Convolucional **conv12**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "\n",
        "### 2.7. Capa Convolucional **conv13**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 2.8. Capa Convolucional **conv14**:\n",
        "\n",
        "* Filtros: 512 filtros de 3x3\n",
        "* Salida: 512 mapas de características de 14x14 (14-3+2+1=14).\n",
        "          14x14x512\n",
        "          padding=1 y strike=1 kernel=3\n",
        "\n",
        "### 2.9. Capa de Subsampling **s4**:\n",
        "\n",
        "* Operación: Subsampling (promedio) con una ventana de 2x2.\n",
        "* Salida: 512 mapas de características de 7x7.\n",
        "          7x7x512\n",
        "\n",
        "### 2.10. Capa Convolucional **conv15**:\n",
        "\n",
        "Filtros: 4096 filtros de 7x7, por tanto 4096 de produnfidad o mapas\n",
        "Salida: 4096 mapas de características de 1x1 (7 - 7 + 1 = 1).\n",
        "       1x1x4096\n",
        "\n",
        "### 2.11. Capa Completamente Conectada **f1**:\n",
        "\n",
        "* Neuronas: 200.\n",
        "* Operación: Función de activación sigmoid o tanh.\n",
        "\n",
        "\n",
        "### 2.12. Capa Completamente Conectada **f2**:\n",
        "\n",
        "* Neuronas: 200.\n",
        "* Operación: Función de activación sigmoid o tanh.\n",
        "\n",
        "### 2.13. Capa Completamente Conectada **f3**:\n",
        "\n",
        "* Neuronas: 200.\n",
        "* Operación: Función de activación sigmoid o tanh.\n",
        "\n",
        "\n",
        "### 2.14 Capa Completamente Conectada **f4** (salida):\n",
        "\n",
        "* Neuronal: 1000 (una por cada dígito del 0 al 999).\n",
        "* Operación: Función de activación **softmax** para clasificación.\n",
        "\n",
        "### 1.2. Flujo de Datos en la Red\n",
        "* Entrada: Imagen de 224x224x3 píxeles.\n",
        "* **conv1**: Convolución → 224x224x64.  64 filtros\n",
        "* **conv2**: Convolución → 224x224x64.\n",
        "* **s1**: Subsampling → 112x112x128.    128 filtros\n",
        "* **conv3**: Convolución → 112x112x128.  128 filtros\n",
        "* **conv4**: Convolución → 112x112x128.  128 filtros\n",
        "* **s2**: Subsampling → 56x56x128.    128 filtros\n",
        "* **conv5**: Convolución → 56x56x256.  256 filtros\n",
        "* **conv6**: Convolución → 56x56x256.  256 filtros\n",
        "* **conv7**: Convolución → 56x56x256.  256 filtros\n",
        "* **s3**: Subsampling → 28x28x512.   512 filtros\n",
        "* **conv8**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv9**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv10**: Convolución → 28x28x512.  512 filtros\n",
        "* **conv11**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv12**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv13**: Convolución → 14x14x512.  512 filtros\n",
        "* **conv14**: Convolución → 14x14x512.  512 filtros\n",
        "* **s4**: Subsampling → 7x7x512.   512 filtros\n",
        "* **conv15**: Convolución → 1x1x4096.  4096 filtros\n",
        "* **f1**: Conexión completa → 200.\n",
        "* **f2**: Conexión completa → 200.\n",
        "* **f3**: Conexión completa → 200.\n",
        "* **f4**: Conexión completa → 1000 (clases)."
      ],
      "metadata": {
        "id": "e_M-8huuJWHs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "De0I2brfyyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        # conv1: entrada 3 canal, salida 6 canales(06 filtros), tamaño de kernel o filtro 5x5\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5,padding=2, stride=1)\n",
        "\n",
        "        # conv2: entrada 6 canales, salida 16 canales, tamaño de kernel 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        return x\n",
        "\n",
        "        # conv3: entrada 16*4*4, salida 120\n",
        "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # f2: entrada 120, salida 84\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "\n",
        "        # Capa completamente conectada 3: entrada 84, salida 10 (dígitos)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicar la primera capa convolucional seguida de ReLU y max pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        # Aplicar la segunda capa convolucional seguida de ReLU y max pooling\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        # Aplicar la tercera capa convolucional seguida de ReLU\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Aplanar los datos para la capa completamente conectada\n",
        "        x = x.view(-1, 120)\n",
        "\n",
        "        # Aplicar la primera capa completamente conectada seguida de ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Aplicar la segunda capa completamente conectada\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # No es necesario aplicar el softmax si se utiliza el \"CrossEntropyLoss\"\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kgTe12RG0Y92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un batch de 100 imágenes de 224x224x3\n",
        "\n",
        "x ="
      ],
      "metadata": {
        "id": "wWU6VCsLy_0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alimentar el modelo VGG16 con el batch de imágenes\n",
        "\n",
        "y = model(x)"
      ],
      "metadata": {
        "id": "VBXqZ70EzDrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el Modelo VGG16\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "\n",
        "model = VGG()"
      ],
      "metadata": {
        "id": "Oi71gHYp1V6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}