{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ricardomanuel1/Maestria_Ciencia_de_Datos/blob/main/MACHINE%20LEARNING%20Y%20DEEP%20LEARNING/3_Afinamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#los parametros en un MLP son la matriz de pesos y los bayas, si yo defino una capa oculta con 30 neuronas y si la entrada es de dimension 100\n",
        "#se va necesitar una matriz de 100x30 para conectar la capa de entrada con la capa oculta\n",
        "#esa matriz es donde ocurre el aprendizaje de una red neuronal MLP, son las matrices las que guardan los pesos\n",
        "#entre capa y capa esos son los parametros en un red neuronal"
      ],
      "metadata": {
        "id": "E2abiSkGokUD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#los hiperpametros el número de capas, el número de neuronas,la funcion de activacion,el optimizador,\n",
        "#tamaños del bacht, el numero de epocas, la tecnica de regularizacion y los valores iniciales de los pesos\n",
        "#antes de comenzar el entrenamiento\n",
        "#los hiperparametros pueden hacer que resulten mejores parametros"
      ],
      "metadata": {
        "id": "dOKVJgJPptBM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Afinamiento de Hiperparámetros en una Red Neuronal MLP**"
      ],
      "metadata": {
        "id": "frNuMy8EMrNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- La flexibilidad de las redes neuronales es también uno de sus principales inconvenientes: hay muchos hiperparámetros que modificar.\n",
        "\n",
        "- Incluso en un MLP básico se puede cambiar el número\n",
        "de capas, el número de neuronas y el tipo de función de activación a utilizar en cada capa, la lógica de inicialización de pesos, el tipo de optimizador a utilizar, su tasa de aprendizaje, tamaño de batch\n",
        "\n",
        "Opciones para está búsqueda:\n",
        "\n",
        "- Convertir el modelo Keras en un estimador Scikit-Learn, y usarGridSearchCV o RandomizedSearchCV para ajustar los\n",
        "hiperparámetros. Para esto, puede usar las\n",
        "clases  KerasRegressor y KerasClassifier de SciKeras ( https://github.com/adriangb/scikeras).\n",
        "\n",
        "- Una mejor opción: usar la biblioteca Keras Tuner, desarrollada para ajustar hiperparámetros en modelos Keras. Esa será la opción mostrada a seguir."
      ],
      "metadata": {
        "id": "6gfJ-orRM-uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 7)"
      ],
      "metadata": {
        "id": "wdIpZrzpM9LN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ],
      "metadata": {
        "id": "Gy8WTxayMqT0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "bewZ-mhCNE6c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "YPDaTN0PNO81"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: MNIST Fashion"
      ],
      "metadata": {
        "id": "306grEAjNXAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7aHQw-6NifO",
        "outputId": "1cbbbe46-ca90-4556-8c0f-6954b600bf36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento, validación, prueba"
      ],
      "metadata": {
        "id": "AnQRbuoAODGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]#\n",
        "\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
      ],
      "metadata": {
        "id": "SpeWfopYMqXg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "IDwNBrAK98YG",
        "outputId": "b6539949-f42a-41e4-d9ee-a50ad0cb4cd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.shape"
      ],
      "metadata": {
        "id": "iQLeISsg-GUh",
        "outputId": "71b96676-54b0-467f-bb0d-944a13356296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "gn8W4RhQ-WSz",
        "outputId": "9593ec50-c990-47be-f279-2d1864cced67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "8EzGyodQMxIy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keras Tuner**"
      ],
      "metadata": {
        "id": "pDhxwtNkfWBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Después de instalar e importar ``keras_tuner``, generalmente como kt, se escribe una función que construye, compila y devuelve un modelo de Keras.\n",
        "- La función debe tomar un objeto kt.HyperParameters como argumento, lo que permite definir hiperparámetros (enteros, flotantes, strings, etc.) junto con sus posibles rangos de valores,\n",
        "- Estos hiperparámetros pueden usarse para construir\n",
        "y compilar el modelo.\n",
        "\n",
        "La siguiente función construye y compila un MLP para clasificar imágenes de MNIST Fashion, se definen hiperparámetros como el número de capas ocultas (``n_hidden``), el número de neuronas por capa (``n_neurons``), la tasa de aprendizaje y el tipo de optimizador a utilizar :"
      ],
      "metadata": {
        "id": "o-lxE8UyZGtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación Keras Tuner"
      ],
      "metadata": {
        "id": "i8dfEeRnOHwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDdKrq6CM1cI",
        "outputId": "fc28f811-4ecf-4a0e-8d35-9cc71909b358"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición de la función"
      ],
      "metadata": {
        "id": "smqRcezyORVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
        "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
        "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
        "                             sampling=\"log\")\n",
        "\n",
        "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))#es de 10 clases y se sa softmax cuando tienen mas de 2 clases\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])#loss=\"sparse_categorical_crossentropy\" se usa por ser de clasificacion\n",
        "    return model"
      ],
      "metadata": {
        "id": "ToYCFSkIOQoO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- La primera parte de la función define los hiperparámetros. Por ejemplo, ``hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)`` comprueba si un hiperparámetro llamado ``\"n_hidden\"``\n",
        "ya está presente en el objeto HyperParameters hp y, de ser así, devuelve su valor. Si no, registra un nuevo hiperparámetro entero llamado ``\"n_hidden\"``, cuyos valores posibles van de 0 a 8 (inclusive), y\n",
        "devuelve el valor predeterminado, que es 2. Cuando ``default`` no es definido, el valor retornado es ``min_value``.\n",
        "- El hiperparámetro \"n_neurons\" es registrado de manera similar.\n",
        "- El hiperparámetro \"learning_rate\" es\n",
        "registrado como flotante que oscila entre $10^{-4}$ a $10^{-2}$, y desde y dado ``sampling=\"log\"``\n",
        "muestreo=\"log\", las tasas de aprendizaje de todas las escalas serán sorteadas por igual.\n",
        "- El hiperparámetro ``optimizer`` se registra con dos posibles\n",
        "valores: ``\"sgd\"`` o ``\"adam\"`` (el valor predeterminado es sgd).\n",
        "\n",
        "- La segunda parte de la función construye el modelo usando los\n",
        "valores de hiperparámetros. Crea un modelo secuencial comenzando con un capa Flatten, seguida del número solicitado de capas ocultas (\n",
        "determinado por el hiperparámetro ``n_hidden``) usando la activación ReLU y una capa de salida con 10 neuronas (una por clase) usando la\n",
        "función de activación softmax. Por último, la función compila el modelo y lo devuelve.\n",
        "\n",
        "\n",
        "Si se desea realizar una búsqueda aleatoria, se puede crear un tuner kt.RandomSearch, pasandole la función build_model al\n",
        "constructor y llamando al método search() del tuner:"
      ],
      "metadata": {
        "id": "N_DPgNhNchkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''objective=\"val_accuracy\" es una metrica que va de cero a 1, uno es perfecto\n",
        "max_trials=5, numero maximo de experimentos\n",
        "overwrite=True, sobreescribe los resultados intermedios de las ejecuciones\n",
        "los resultados seran grabados en la carpeta my_fashion_mnist'''"
      ],
      "metadata": {
        "id": "qgZqLLsPE_Zk",
        "outputId": "e1fe5315-41ae-4bde-c7f7-4d7b9987a985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'objective=\"val_accuracy\" es una metrica que va de cero a 1, uno es perfecto\\nmax_trials=5, numero maximo de experimentos\\noverwrite=True, sobreescribe los resultados intermedios de las ejecuciones\\nlos resultados seran grabados en la carpeta my_fashion_mnist'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cuando termine el entrenamiento tendremos el mejor modelo"
      ],
      "metadata": {
        "id": "QXve5HfGGOip"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search_tuner = kt.RandomSearch(\n",
        "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
        "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)#max_trials=5 experiemntos\n",
        "\n",
        "random_search_tuner.search(X_train, y_train, epochs=10,\n",
        "                           validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0R9qNdMOVDO",
        "outputId": "6b632ec8-7ad8-4046-b2f3-f18596d7f09e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 01m 13s]\n",
            "val_accuracy: 0.8402000069618225\n",
            "\n",
            "Best val_accuracy So Far: 0.8695999979972839\n",
            "Total elapsed time: 00h 07m 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El tuner ``RandomSearch`` llama a build_model() una vez con un\n",
        "objeto de hiperparámetros vacío, solo para reunir sus especificaciones.\n",
        "- Luego, en este ejemplo, se ejecuta 5 veces; en cada trial, construye un modelo utilizando hiperparámetros muestreados aleatoriamente dentro de sus respectivos rangos, luego entrena ese modelo durante 10 épocas y lo guarda en un subdirectorio.\n",
        "- Dado que ``overwrite=True``, el directorio my_rnd_search se elimina antes de iniciar el entrenamiento. Si se ejecuta este código por segunda vez pero con ``overwrite=False`` y ``max_trials=10``, el tuner continuará el afinamiento desde donde se quedó, ejecutando 5 pruebas más\n",
        "- Por último, dado que el objetivo es \"val_accuracy\", el tuner prefiere modelos con una accuracy alto en la  validación, por lo que una vez que el tuner termina, se podrá obtener los mejores modelos:"
      ],
      "metadata": {
        "id": "48sRm3Loc7r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#no es usual hacer croos validacion con modelos profundos"
      ],
      "metadata": {
        "id": "mewjjMUVGaS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top3_models = random_search_tuner.get_best_models(num_models=3)#permite encontrar los 3 mejores modelos\n",
        "best_model = top3_models[0]#muestra el primer modelo"
      ],
      "metadata": {
        "id": "DozaFDCcPR6s",
        "outputId": "282c2665-5f07-4ff2-c344-fb09cef16957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-3.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-3.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-5.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-5.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-3.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-3.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-5.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-5.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-6.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-6.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-7.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-7.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También se puede llamar a get_best_hyperparameters() para obtener\n",
        "kt.HyperParameters de los mejores modelos:"
      ],
      "metadata": {
        "id": "Tzc-zvE6dQIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)#parametros de los 3 modelos\n",
        "top3_params[0].values  # best hyperparameter values del primer modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOsHfRZGPUx0",
        "outputId": "7036f075-8652-4914-da97-fd2e53f1714d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_hidden': 8,\n",
              " 'n_neurons': 37,\n",
              " 'learning_rate': 0.008547485565344062,\n",
              " 'optimizer': 'sgd'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cada tuner es guiado por un oráculo: antes de cada prueba, el tuner pregunta al oráculo que le indique cuál debería ser la próxima prueba.\n",
        "- El tuner RandomSearch utiliza RandomSearchOracle, que es bastante básico: simplemente selecciona el siguiente trial al azar.\n",
        "- Dado que el oráculo realiza un seguimiento de todas\n",
        "las pruebas, se puede solicitar la mejor y mostrar un resumen:\n"
      ],
      "metadata": {
        "id": "6qiDPdg1da5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
        "best_trial.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-FLvAhdPXdc",
        "outputId": "455d43ce-6f26-499e-ad3f-10718e929d6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "n_hidden: 8\n",
            "n_neurons: 37\n",
            "learning_rate: 0.008547485565344062\n",
            "optimizer: sgd\n",
            "Score: 0.8695999979972839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También se puede acceder a las métricas directamente:"
      ],
      "metadata": {
        "id": "18S0avZfdjJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial.metrics.get_last_value(\"val_accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09okbKrLPa3G",
        "outputId": "8da2a6f1-3248-4ed0-ae69-36de99f44bac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8695999979972839"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ya encontrando los mejores hiperparametros es necesario entrenarlo con el full data"
      ],
      "metadata": {
        "id": "1sNK8kR5-gmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento completo\n",
        "\n",
        "Si se está satisfecho con el rendimiento del mejor modelo, se puede continuar entrenándolo durante algunas épocas usando el conjunto de entrenamiento completo (X_train_full y\n",
        "y_train_full), para despúes evaluarlo en el conjunto de prueba y ponerlo en producción."
      ],
      "metadata": {
        "id": "zpqg8STnPbp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jRhBnTEPdKa",
        "outputId": "b0f4e3b7-f164-4461-e451-0967153993c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.3613 - accuracy: 0.8690\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3528 - accuracy: 0.8710\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3444 - accuracy: 0.8739\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3393 - accuracy: 0.8768\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3373 - accuracy: 0.8765\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3325 - accuracy: 0.8787\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3273 - accuracy: 0.8798\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3214 - accuracy: 0.8820\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3162 - accuracy: 0.8833\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3126 - accuracy: 0.8851\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3832 - accuracy: 0.8638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Elección de parámetros**"
      ],
      "metadata": {
        "id": "1fS2WzPs9wH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de Capas Ocultas**\n",
        "- Para muchos problemas se puede empezar con sólo una o dos capas ocultas.\n",
        "\n",
        "- Se puede alcanzar fácilmente una precisión superior al 97 % en el conjunto de datos MNIST utilizando solo una\n",
        "capa oculta con unos pocos cientos de neuronas y una precisión superior al 98% utilizando\n",
        "dos capas ocultas con el mismo número de neuronas.\n",
        "\n",
        "- Para problemas más complejos, se puede aumentar el número de capas ocultas hasta que produzca overfitting en el conjunto de entrenamiento.\n",
        "\n",
        "- Tareas muy complejas, como clasificación de imágenes o reconocimiento de voz normalmente requieren redes con docenas de capas (o incluso cientos, pero no MLPs), y necesitan una gran cantidad de datos de entrenamiento.\n",
        "\n",
        "- Transfer learning: rara vez se entrena tales\n",
        "redes grandes desde cero: es mucho más común reutilizar partes de una\n",
        "red de última generación previamente entrenada que realiza una tarea similar. La clasificación entonces será mucho más rápida y requerirá muchos menos datos.\n"
      ],
      "metadata": {
        "id": "UIUS5yie94cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de neuronas por capa oculta**\n",
        "\n",
        "- Las capas de entrada y salida dependen de los datos\n",
        "- Antes se pensaba que en MLPs debería comenzarse con muchas neuronas e ir disminuyendo gradualmente. La red neuronal para MNIST podía tener 3 capas ocultas, la primera con 300 neuronas, la segunda con 200 y la tercera con 100.\n",
        "- Esta práctica ha sido abandonada en gran medida porque parece que usar el mismo número de neuronas en todas las capas ocultas funciona igual de bien en la mayoría de casos.\n",
        "-  En general, dependiendo del conjunto de datos, a veces puede resultar útil hacer la primera capa oculta más grande que las demás."
      ],
      "metadata": {
        "id": "72rAbQRM94oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate, Batch Size, y otros**\n",
        "\n",
        "*Learning rate*:\n",
        "\n",
        "- Para encontrar una buena tasa de aprendizaje se entrena\n",
        "el modelo durante unos cientos de iteraciones, comenzando con un valor muy bajo (por ejemplo, $10^{-5}$)\n",
        "- Se aumenta gradualmente hasta un valor muy grande (por ejemplo,\n",
        "10).\n",
        "- Esto se consigue multiplicando la tasa de aprendizaje por un factor constante en cada iteración (por ejemplo, por $(10/10^{-5})^{1/500}$ para pasar de $10^{-5}$ a 10 en 500\n",
        "iteraciones).\n",
        "- Se grafica la pérdida (loss) en función de la tasa de aprendizaje (usando un escala logarítmica), se debería observar que disminuye al principio. Pero después de un tiempo, la tasa de aprendizaje será demasiado grande, por lo que el loss se disparará.\n",
        "- Copia de seguridad: la tasa de aprendizaje óptima será un poco más baja que el punto en donde el loss comienza a aumentar.\n",
        "- Se re entrena el modelo con esta tasa de aprendizaje óptima.\n",
        "\n",
        "*Optimizador*:\n",
        "\n",
        "Elegir un optimizador mejor que el antiguo descenso de gradiente con mini batches (y ajustar sus hiperparámetros) es bastante importante. Se verán opciones en otros jupyters.\n",
        "\n",
        "*Batch*:\n",
        "\n",
        "- El tamaño del batch puede tener un impacto significativo en el rendimiento de su modelo.\n",
        "- El principal beneficio de utilizar batches grandes es que las GPU pueden procesarlos eficientemente.\n",
        "- Sin embargo, hay un problema: en la práctica, los batches grandes a menudo conducen a inestabilidades del entrenamiento, especialmente al inicio del mismo, y el modelo resultante puede no generalizar bien\n",
        "- Según LeCun (2018) se debe usar tamaños de batch de hasta 32.\n",
        "- Dominic Masters y Carlo Luschi, investigaron que el uso de pequeños\n",
        "batches (de 2 a 32) era preferible ya que conducían a mejores modelos en menos tiempo de entrenamiento.\n",
        "-  Para contrariar esto, Elad Hoffer et al.\n",
        "y Priya Goyal et al. demostraron que era posible utilizar tamaños muy grandes de batch (hasta 8,192) junto con varias técnicas asociadas a la alteración de la tasa de aprendizaje. Obtuvieron tiempos de entrenamiento cortos, sin ninguna brecha de generalización.\n",
        "- Una estrategia es intentar a utilizar un tamaño de batch grande, con una pequeña tasa de aprendizaje y luego aumentándola, y si el entrenamiento es inestable o el rendimiento final es decepcionante, utilice un batch pequeño.\n",
        "\n",
        "*Función de activación*\n",
        "\n",
        "En general, la función de activación ReLU será un buen valor predeterminado para todas las capas ocultas, pero para la capa de salida depende de la tarea a resolver.\n",
        "\n",
        "*Número de iteraciones*\n",
        "\n",
        "En la mayoría de los casos, no es necesario que el número de iteraciones sea ajustada, utilice early stopping."
      ],
      "metadata": {
        "id": "iOrius8YA2hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#batch cuantos ejemplos se va utlizar para actualizar"
      ],
      "metadata": {
        "id": "VdUmy9Wf_NM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}